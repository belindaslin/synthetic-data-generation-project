# -*- coding: utf-8 -*-
"""aids-sick-data

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Gjukg8ND4pDzMgBksxFxh45JWKtJXiSv
"""

!pip install "git+https://github.com/vkinakh/binary-diffusion-tabular.git" \
             huggingface_hub joblib pyyaml

import yaml
with open(config_path, "r") as f:
    sick_cfg = yaml.safe_load(f)

sick_cfg

import torch, yaml, joblib
import pandas as pd
from huggingface_hub import hf_hub_download
from binary_diffusion_tabular import (
    FixedSizeBinaryTableDataset,
    drop_fill_na,
    SimpleTableGenerator,
    BinaryDiffusion1D,
    FixedSizeTableBinaryDiffusionTrainer,
)

device = "cuda" if torch.cuda.is_available() else "cpu"

# ---- download Sick artifacts from HF ----
repo_id = "vitaliykinakh/binary-ddpm-tabular"
subdir  = "sick"

ckpt_path      = hf_hub_download(repo_id, filename=f"{subdir}/model-final.pt")
config_path    = hf_hub_download(repo_id, filename=f"{subdir}/config.yaml")
transform_path = hf_hub_download(repo_id, filename=f"{subdir}/transformation.joblib")

print("checkpoint:", ckpt_path)
print("config:    ", config_path)
print("transform: ", transform_path)

# ---- load config ----
with open(config_path, "r") as f:
    cfg = yaml.safe_load(f)

data_cfg    = cfg["data"]
model_cfg   = cfg["model"]
diff_cfg    = cfg["diffusion"]
trainer_cfg = cfg["trainer"]

# ---- load real Sick dataset ----
sick_url = "https://raw.githubusercontent.com/vkinakh/binary-diffusion-tabular/main/data/sick_train.csv"
df = pd.read_csv(sick_url)

# drop columns specified in config (TBG, TBG_measured)
cols_to_drop = data_cfg.get("columns_to_drop")
if cols_to_drop:
    df = df.drop(columns=cols_to_drop)

# column *names* from config (keep them as lists of strings, do NOT slice df here)
columns_numerical   = data_cfg["numerical_columns"]
columns_categorical = data_cfg["categorical_columns"]
target_col          = data_cfg["target_column"]
task                = data_cfg["task"]

# same NA handling as original training
df = drop_fill_na(
    df=df,
    columns_numerical=columns_numerical,
    columns_categorical=columns_categorical,
    dropna=data_cfg["dropna"],
    fillna=data_cfg["fillna"],
)

# build dataset
dataset = FixedSizeBinaryTableDataset(
    table=df,
    target_column=target_col,
    split_feature_target=data_cfg["split_feature_target"],
    task=task,
    numerical_columns=columns_numerical,
    categorical_columns=columns_categorical,
)

print("row_size:", dataset.row_size, "n_classes:", getattr(dataset, "n_classes", None))

# ---- build model + diffusion ----
target_diffusion         = diff_cfg["target"]              # "two_way"
classifier_free_guidance = trainer_cfg["classifier_free_guidance"]

model = SimpleTableGenerator(
    data_dim=dataset.row_size,
    dim=model_cfg["dim"],
    n_res_blocks=model_cfg["n_res_blocks"],
    out_dim=(
        dataset.row_size * 2
        if target_diffusion == "two_way"
        else dataset.row_size
    ),
    task=task,
    conditional=dataset.conditional,
    n_classes=0 if task == "regression" else dataset.n_classes,
    classifier_free_guidance=classifier_free_guidance,
).to(device)

diffusion = BinaryDiffusion1D(
    denoise_model=model,
    schedule=diff_cfg["schedule"],
    n_timesteps=diff_cfg["n_timesteps"],
    target=target_diffusion,
).to(device)

# ---- load pretrained Sick checkpoint ----
state = torch.load(ckpt_path, map_location=device)
print("checkpoint keys:", state.keys())

if "diffusion" in state:
    diffusion.load_state_dict(state["diffusion"])
elif "model" in state:
    model.load_state_dict(state["model"])
else:
    model.load_state_dict(state)

# ---- dummy logger so trainer doesn't crash ----
class NoOpLogger:
    def log(self, *args, **kwargs):
        pass

logger = NoOpLogger()

# ---- finetuning settings ----
finetune_steps = 10_000   # shrink to 10_000 if you want it faster
finetune_lr    = 1e-5
results_folder = "./results/sick_finetune"

trainer = FixedSizeTableBinaryDiffusionTrainer(
    diffusion=diffusion,
    dataset=dataset,
    train_num_steps=finetune_steps,
    log_every=trainer_cfg["log_every"],
    save_every=trainer_cfg["save_every"],
    save_num_samples=trainer_cfg["save_num_samples"],
    max_grad_norm=trainer_cfg["max_grad_norm"],
    gradient_accumulate_every=trainer_cfg["gradient_accumulate_every"],
    ema_decay=trainer_cfg["ema_decay"],
    ema_update_every=trainer_cfg["ema_update_every"],
    lr=finetune_lr,
    opt_type=trainer_cfg["opt_type"],
    batch_size=trainer_cfg["batch_size"],
    dataloader_workers=trainer_cfg["dataloader_workers"],
    classifier_free_guidance=classifier_free_guidance,
    zero_token_probability=trainer_cfg["zero_token_probability"],
    logger=logger,
    results_folder=results_folder,
)

trainer.train()

import glob, pandas as pd

results_folder = "./results/sick_finetune"

sample_files = sorted(glob.glob(f"{results_folder}/samples*.csv"))
print("Found", len(sample_files), "sample files")

df_all = pd.concat([pd.read_csv(f) for f in sample_files], ignore_index=True)
df_all.to_csv("synthetic_all_samples.csv", index=False)
print("Saved synthetic_all_samples.csv with", len(df_all), "rows")

import glob, pandas as pd

results_folder = "./results/sick_finetune"   # or travel_finetune, etc.

# grab all ema sample files
ema_files = sorted(glob.glob(f"{results_folder}/samples*_ema.csv"))
print("Found", len(ema_files), "EMA files")

# read + stack into one dataframe
df_ema = pd.concat([pd.read_csv(f) for f in ema_files], ignore_index=True)

# save single big CSV
df_ema.to_csv("synthetic_all_ema.csv", index=False)
print("Saved synthetic_all_ema.csv with", len(df_ema), "rows")

import pandas as pd

# Option A — load from GitHub
df = pd.read_csv("https://raw.githubusercontent.com/vkinakh/binary-diffusion-tabular/main/data/sick_train.csv")

# Option B — if you downloaded/uploaded the file locally:
# df = pd.read_csv("sick_train.csv")

print("Number of rows:", df.shape[0])
print("Number of columns:", df.shape[1])

!wget -O sample.py https://raw.githubusercontent.com/vkinakh/binary-diffusion-tabular/main/sample.py

import glob
glob.glob("./results/sick_finetune/model-*.pt")

finetuned_ckpt = "./results/sick_finetune/model-final.pt"
travel_transform_path = "./results/sick_finetune/transformation.joblib"
out_dir = "./synthetic_sick_full"

!python sample.py \
  --ckpt={finetuned_ckpt} \
  --ckpt_transformation={travel_transform_path} \
  --n_timesteps=1000 \
  --out={out_dir} \
  --n_samples=3018 \
  --batch_size=512 \
  --threshold=0.5 \
  --strategy=target \
  --guidance_scale=0.0 \
  --target_column_name=Target \
  --device=cpu

import pandas as pd
import glob
import os

out_dir = "./synthetic_sick_full"   # same as --out in sample.py

# Read all CSV files produced by sample.py
csv_files = sorted(glob.glob(os.path.join(out_dir, "*.csv")))
print("Found files:", csv_files)

dfs = [pd.read_csv(f) for f in csv_files]
full_df = pd.concat(dfs, ignore_index=True)

# Make sure we have exactly 3018 rows
full_df = full_df.head(3018)       # or .sample(3018, random_state=0) if you want random subset

# Save single CSV
out_path = os.path.join(out_dir, "synthetic_sick_3018.csv")
full_df.to_csv(out_path, index=False)

print("Saved:", out_path, "with shape", full_df.shape)