# -*- coding: utf-8 -*-
"""aids-housing-data

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eB-7k-QbiQvbEd6gZGuUy1O73NvyyFd-
"""

!pip install "git+https://github.com/vkinakh/binary-diffusion-tabular.git" \
             huggingface_hub joblib pyyaml



import torch, yaml, joblib
import pandas as pd
from huggingface_hub import hf_hub_download

from binary_diffusion_tabular import (
    FixedSizeBinaryTableDataset,
    drop_fill_na,
    SimpleTableGenerator,
    BinaryDiffusion1D,
    FixedSizeTableBinaryDiffusionTrainer,
)

device = "cuda" if torch.cuda.is_available() else "cpu"
device

repo_id = "vitaliykinakh/binary-ddpm-tabular"
subdir  = "housing"   # folder name on HF

ckpt_path      = hf_hub_download(repo_id, filename=f"{subdir}/model-final.pt")
config_path    = hf_hub_download(repo_id, filename=f"{subdir}/config.yaml")
transform_path = hf_hub_download(repo_id, filename=f"{subdir}/transformation.joblib")

print("ckpt:", ckpt_path)
print("config:", config_path)
print("transform:", transform_path)

with open(config_path, "r") as f:
    cfg = yaml.safe_load(f)

data_cfg    = cfg["data"]
model_cfg   = cfg["model"]
diff_cfg    = cfg["diffusion"]
trainer_cfg = cfg["trainer"]

housing_url = "https://raw.githubusercontent.com/vkinakh/binary-diffusion-tabular/main/data/housing_train.csv"
df = pd.read_csv(housing_url)

n_rows = df.shape[0]
print("Housing rows:", n_rows)

columns_numerical   = data_cfg["numerical_columns"]
columns_categorical = data_cfg["categorical_columns"]
target_col          = data_cfg["target_column"]
task                = data_cfg["task"]

df = drop_fill_na(
    df=df,
    columns_numerical=columns_numerical,
    columns_categorical=columns_categorical,
    dropna=data_cfg["dropna"],
    fillna=data_cfg["fillna"],
)

dataset = FixedSizeBinaryTableDataset(
    table=df,
    target_column=target_col,
    split_feature_target=data_cfg["split_feature_target"],
    task=task,
    numerical_columns=columns_numerical,
    categorical_columns=columns_categorical,
)

print("row_size:", dataset.row_size)

device = "cuda" if torch.cuda.is_available() else "cpu"

model = SimpleTableGenerator(
    data_dim=dataset.row_size,
    dim=model_cfg["dim"],
    n_res_blocks=model_cfg["n_res_blocks"],
    out_dim=dataset.row_size * 2 if target_diffusion == "two_way" else dataset.row_size,
    task=task,
    conditional=dataset.conditional,
    n_classes=0 if task == "regression" else dataset.n_classes,
    classifier_free_guidance=trainer_cfg["classifier_free_guidance"],
).to(device)

diffusion = BinaryDiffusion1D(
    denoise_model=model,
    schedule=diff_cfg["schedule"],
    n_timesteps=diff_cfg["n_timesteps"],
    target=target_diffusion,
).to(device)

class NoOpLogger:
    def log(self, *args, **kwargs): pass

logger = NoOpLogger()

results_folder = "./results/housing_finetune"
finetune_steps = 10_000   # reduce if needed
finetune_lr    = 1e-5

trainer = FixedSizeTableBinaryDiffusionTrainer(
    diffusion=diffusion,
    dataset=dataset,
    train_num_steps=finetune_steps,
    log_every=trainer_cfg["log_every"],
    save_every=trainer_cfg["save_every"],
    save_num_samples=trainer_cfg["save_num_samples"],
    max_grad_norm=trainer_cfg["max_grad_norm"],
    gradient_accumulate_every=trainer_cfg["gradient_accumulate_every"],
    ema_decay=trainer_cfg["ema_decay"],
    ema_update_every=trainer_cfg["ema_update_every"],
    lr=finetune_lr,
    opt_type=trainer_cfg["opt_type"],
    batch_size=trainer_cfg["batch_size"],
    dataloader_workers=trainer_cfg["dataloader_workers"],
    classifier_free_guidance=classifier_free_guidance,
    zero_token_probability=trainer_cfg["zero_token_probability"],
    logger=logger,
    results_folder=results_folder,
)

trainer.train()

!wget -O sample.py https://raw.githubusercontent.com/vkinakh/binary-diffusion-tabular/main/sample.py

import glob
finetuned_ckpt = sorted(glob.glob("./results/housing_finetune/model-*.pt"))[-1]
finetuned_ckpt

out_dir = "./synthetic_housing"

!python sample.py \
    --ckpt="{finetuned_ckpt}" \
    --ckpt_transformation="{transform_path}" \
    --n_timesteps=1000 \
    --out="{out_dir}" \
    --n_samples={16512} \
    --batch_size=512 \
    --threshold=0.5 \
    --strategy=target \
    --guidance_scale=0.0 \
    --target_column_name={target_col} \
    --device=cpu \

import pandas as pd
import glob
import os

out_dir = "./synthetic_housing"   # same as --out in sample.py

# Read all CSV files produced by sample.py
csv_files = sorted(glob.glob(os.path.join(out_dir, "*.csv")))
print("Found files:", csv_files)

dfs = [pd.read_csv(f) for f in csv_files]
full_df = pd.concat(dfs, ignore_index=True)

# Make sure we have exactly 3018 rows
full_df = full_df.head(16512)       # or .sample(3018, random_state=0) if you want random subset

# Save single CSV
out_path = os.path.join(out_dir, "synthetic_housing_16512.csv")
full_df.to_csv(out_path, index=False)

print("Saved:", out_path, "with shape", full_df.shape)