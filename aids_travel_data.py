# -*- coding: utf-8 -*-
"""aids-travel-data

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1onvW21XRSD9LySrWOH43yQo078fX7DbH
"""

!pip install "git+https://github.com/vkinakh/binary-diffusion-tabular.git"
!pip install huggingface_hub joblib pyyaml

from huggingface_hub import hf_hub_download

repo_id = "vitaliykinakh/binary-ddpm-tabular"
subdir = "travel"

ckpt_path = hf_hub_download(repo_id, filename=f"{subdir}/model-final.pt")
config_path = hf_hub_download(repo_id, filename=f"{subdir}/config.yaml")
transform_path = hf_hub_download(repo_id, filename=f"{subdir}/transformation.joblib")

ckpt_path, config_path, transform_path

import yaml

with open(config_path, "r") as f:
    cfg = yaml.safe_load(f)

cfg

import pandas as pd
from binary_diffusion_tabular import (
    FixedSizeBinaryTableDataset,
    drop_fill_na,
)

data_cfg = cfg["data"]

# 1) read data: either the original Travel csv, or your own csv with SAME columns
df = pd.read_csv("travel_train.csv")  # or your own file

# 2) pull the correct fields from config
columns_numerical   = data_cfg["numerical_columns"]
columns_categorical = data_cfg["categorical_columns"]
task       = data_cfg["task"]            # "classification"
target_col = data_cfg["target_column"]   # "Target"

# 3) apply the same NA handling as in config
df = drop_fill_na(
    df=df,
    columns_numerical=columns_numerical,
    columns_categorical=columns_categorical,
    dropna=data_cfg["dropna"],
    fillna=data_cfg["fillna"],
)

# 4) build the dataset
dataset = FixedSizeBinaryTableDataset(
    table=df,
    target_column=target_col,
    split_feature_target=data_cfg["split_feature_target"],
    task=task,
    numerical_columns=columns_numerical,
    categorical_columns=columns_categorical,
)

import yaml

with open(config_path, "r") as f:
    cfg = yaml.safe_load(f)

cfg

import torch
from binary_diffusion_tabular import (
    SimpleTableGenerator,
    BinaryDiffusion1D,
    FixedSizeTableBinaryDiffusionTrainer,
)

# ---------- config + device ----------
data_cfg    = cfg["data"]
model_cfg   = cfg["model"]
diff_cfg    = cfg["diffusion"]
trainer_cfg = cfg["trainer"]

task                = data_cfg["task"]              # "classification"
target_diffusion    = diff_cfg["target"]           # "two_way"
classifier_free_guidance = trainer_cfg["classifier_free_guidance"]

device = "cuda" if torch.cuda.is_available() else "cpu"

# ---------- build model ----------
model = SimpleTableGenerator(
    data_dim=dataset.row_size,
    dim=model_cfg["dim"],
    n_res_blocks=model_cfg["n_res_blocks"],
    out_dim=(
        dataset.row_size * 2
        if target_diffusion == "two_way"
        else dataset.row_size
    ),
    task=task,
    conditional=dataset.conditional,
    n_classes=0 if task == "regression" else dataset.n_classes,
    classifier_free_guidance=classifier_free_guidance,
).to(device)

# ---------- diffusion wrapper ----------
diffusion = BinaryDiffusion1D(
    denoise_model=model,
    schedule=diff_cfg["schedule"],
    n_timesteps=diff_cfg["n_timesteps"],
    target=target_diffusion,
).to(device)

# ---------- load pretrained checkpoint ----------
state = torch.load(ckpt_path, map_location=device)
print("Checkpoint keys:", state.keys())

# In this checkpoint, weights live under "diffusion"
if "diffusion" in state:
    diffusion.load_state_dict(state["diffusion"])
elif "model" in state:
    model.load_state_dict(state["model"])
else:
    # fallback: assume the dict is directly the model state
    model.load_state_dict(state)

# ---------- dummy logger to avoid AttributeError ----------
class NoOpLogger:
    def log(self, *args, **kwargs):
        pass

logger = NoOpLogger()

# ---------- trainer for finetuning ----------
finetune_steps = 10_000
finetune_lr    = 1e-5
results_folder = "./results/travel_finetune"

trainer = FixedSizeTableBinaryDiffusionTrainer(
    diffusion=diffusion,
    dataset=dataset,
    train_num_steps=finetune_steps,
    log_every=trainer_cfg["log_every"],
    save_every=trainer_cfg["save_every"],
    save_num_samples=trainer_cfg["save_num_samples"],
    max_grad_norm=trainer_cfg["max_grad_norm"],
    gradient_accumulate_every=trainer_cfg["gradient_accumulate_every"],
    ema_decay=trainer_cfg["ema_decay"],
    ema_update_every=trainer_cfg["ema_update_every"],
    lr=finetune_lr,
    opt_type=trainer_cfg["opt_type"],
    batch_size=trainer_cfg["batch_size"],
    dataloader_workers=trainer_cfg["dataloader_workers"],
    classifier_free_guidance=classifier_free_guidance,
    zero_token_probability=trainer_cfg["zero_token_probability"],
    logger=logger,
    results_folder=results_folder,
)

# ---------- run finetuning ----------
trainer.train()

import glob

glob.glob("./results/travel_finetune/model-*.pt")

from huggingface_hub import hf_hub_download

travel_transform_path = hf_hub_download(
    "vitaliykinakh/binary-ddpm-tabular",
    filename="travel/transformation.joblib"
)

# (re)download sampling script into /content/sample.py
!wget -O sample.py https://raw.githubusercontent.com/vkinakh/binary-diffusion-tabular/main/sample.py

import pandas as pd

travel_url = "https://raw.githubusercontent.com/vkinakh/binary-diffusion-tabular/main/data/travel_train.csv"
df_travel = pd.read_csv(travel_url)
n_rows = df_travel.shape[0]
print(n_rows)  # just to check, e.g. 3200

finetuned_ckpt = "./results/travel_finetune/model-107.pt"
out_dir = "./synthetic_travel_full"

!python sample.py \
  --ckpt="{finetuned_ckpt}" \
  --ckpt_transformation="{travel_transform_path}" \
  --n_timesteps=1000 \
  --out="{out_dir}" \
  --n_samples=763 \
  --batch_size=512 \
  --threshold=0.5 \
  --strategy=target \
  --guidance_scale=0.0 \
  --target_column_name=Target \
  --device=cpu

import glob, pandas as pd

csv_files = glob.glob(f"{out_dir}/*.csv")
print(csv_files)

df_travel_synth = pd.read_csv(csv_files[0])
print(df_travel_synth.shape)  # should be (763, ...)
df_travel_synth.to_csv("synthetic_travel_full.csv", index=False)

import pandas as pd

n_rows = 763  # original travel_train size

df_travel_synth = pd.read_csv("./synthetic_travel_full/samples.csv")
print(df_travel_synth.shape)  # (1024, 7)

# keep only the first n_rows
df_travel_synth = df_travel_synth.iloc[:n_rows].reset_index(drop=True)
print(df_travel_synth.shape)  # (763, 7)

df_travel_synth.to_csv("synthetic_travel_full_763.csv", index=False)